{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\isacm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\isacm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "negation = set([\"aren't\", \"isn't\", \"wasn't\", \"weren't\", \"can't\", \"couldn't\", \n",
    "    \"mustn't\", \"shouldn't\", \"won't\", \"wouldn't\", \"didn't\", \"doesn't\", \"don't\", \n",
    "    \"hasn't\", \"haven't\", \"hadn't\", \"not\"])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_punctuation(str_to_alter):\n",
    "    \"\"\"\n",
    "    Takes a string as input and removes punctuation\n",
    "    \"\"\"\n",
    "    global exclude\n",
    "    new_string = ''.join(ch for ch in str_to_alter if ch not in exclude)\n",
    "\n",
    "    return new_string\n",
    "\n",
    "def has_digits(s):\n",
    "    \"\"\"\n",
    "    Returns true if string has digits.\n",
    "    \"\"\"\n",
    "    return any(ch.isdigit() for ch in s)\n",
    "\n",
    "def is_negation(s):\n",
    "    \"\"\"\n",
    "    Returns True if word will negate coming word \n",
    "    \"\"\"\n",
    "    global negation\n",
    "\n",
    "    return s in negation\n",
    "\n",
    "def is_stopword(s):\n",
    "    \"\"\"\n",
    "    Returns True if the word is in NLTK's list of stop words.\n",
    "    \"\"\"\n",
    "    global stop_words\n",
    "\n",
    "    return s in stop_words\n",
    "\n",
    "class Sample:\n",
    "    \"\"\"\n",
    "    Class for holding samples from dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, body, target_class, preprocess=True):\n",
    "        \"\"\"\n",
    "        Inits and preprocesses samples\n",
    "        \"\"\"\n",
    "        self.original = body\n",
    "        self.body = body\n",
    "        self.length = len(body)\n",
    "        self.target_class = int(target_class)\n",
    "        self.exclamation_point_count = self.body.count('!')\n",
    "        self.question_mark_count = self.body.count('?')\n",
    "        self.tokens = list()\n",
    "\n",
    "        word_tokens = word_tokenize(self.body) \n",
    "        \n",
    "        if preprocess:\n",
    "\n",
    "            ps =PorterStemmer()\n",
    "            negation = False\n",
    "            for w in word_tokens:\n",
    "                if 'http' in w or '//' in w or '.com' in w:\n",
    "                    negation = not negation\n",
    "                    continue\n",
    "                if '&' in w or '#' in w:\n",
    "                    negation = not negation\n",
    "                    continue\n",
    "                root_word=ps.stem(w)\n",
    "                if root_word == \"n't\" or root_word == 'not':\n",
    "                    negation = not negation\n",
    "                    continue\n",
    "                root_word = remove_punctuation(root_word)\n",
    "                if root_word == 'br':\n",
    "                    negation = False\n",
    "                    continue\n",
    "                elif len(root_word) < 2:\n",
    "                    negation = False\n",
    "                    continue\n",
    "                elif has_digits(root_word):\n",
    "                    negation = False\n",
    "                    continue\n",
    "                elif is_stopword(root_word):\n",
    "                    negation = False\n",
    "                    continue\n",
    "                if negation: \n",
    "                    root_word = \"\".join(('!',root_word))\n",
    "                    negation = False\n",
    "                self.tokens.append(root_word)\n",
    "\n",
    "            self.body = \" \".join(self.tokens)\n",
    "\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Prints first 100 characters of the body if object is printed\n",
    "        \"\"\"\n",
    "        return self.body if len(self.body) < 100 else self.body[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to folder with dataset\n",
    "path = \"./data/processed/\"\n",
    "\n",
    "# Lists all files in folder \n",
    "files = listdir(path)[:3]\n",
    "\n",
    "df = []\n",
    "\n",
    "for file in files:\n",
    "    # Read dataset into dataframe and append to list\n",
    "    df.append((file, pd.read_csv(f\"{path}{file}\", sep=\"\\t\")))\n",
    "\n",
    "# For holding dataset with preprocessing\n",
    "file_content = []\n",
    "# For holding dataset without preprocessing\n",
    "file_content_original = []\n",
    "# For training\n",
    "ds = []\n",
    "# For testing\n",
    "ts = []\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "test_set_probability = .25\n",
    "\n",
    "for file, dataset in df:\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    x_train_raw = []\n",
    "    x_test_raw = []\n",
    "    for b, s in zip(dataset['review_body'], dataset['star_rating']):\n",
    "        if random.random() <= test_set_probability:\n",
    "            # if true add to test set\n",
    "            train = True\n",
    "        else:\n",
    "            train=False\n",
    "        # Create object with preprocessing from review\n",
    "        x = Sample(b, 1 if int(s) == 5 else 0)\n",
    "        if train:\n",
    "            ds.append(x)\n",
    "            x_train_raw.append(x.body.strip())\n",
    "            y_train.append(x.target_class)\n",
    "        else:\n",
    "            ts.append(x)\n",
    "            x_test_raw.append(x.body.strip())\n",
    "            y_test.append(x.target_class)\n",
    "    # Append to list\n",
    "    file_content.append((file, ds, x_train_raw, y_train, ts, x_test_raw, y_test))\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    x_train_raw = []\n",
    "    x_test_raw = []\n",
    "    for b, s in zip(dataset['review_body'], dataset['star_rating']):\n",
    "        if random.random() <= .25:\n",
    "            train = True\n",
    "        else:\n",
    "            train=False\n",
    "        # Create object without preprocessing from review\n",
    "        x = Sample(b, 1 if int(s) == 5 else 0, False)\n",
    "        if train:\n",
    "            x_train_raw.append(x.original.strip())\n",
    "            y_train.append(x.target_class)\n",
    "        else:\n",
    "            x_test_raw.append(x.original.strip())\n",
    "            y_test.append(x.target_class)\n",
    "    file_content_original.append((file, ds, x_train_raw, y_train, ts, x_test_raw, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tweets into dataframe\n",
    "df = pd.read_csv(f\"./data/tweets.csv\")\n",
    "\n",
    "x_tweets = []\n",
    "y_tweets = []\n",
    "\n",
    "random.seed(0)\n",
    "# Create dataset for testing\n",
    "for b, s in zip(df['text'], df['sentiment']):\n",
    "    if not isinstance(b, str):\n",
    "        break\n",
    "    if s == 'neutral':\n",
    "        continue\n",
    "    x = Sample(b, 1 if s == 'positive' else 0)\n",
    "    x_tweets.append(x.body.strip())\n",
    "    y_tweets.append(x.target_class)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class QuestionMarkExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts number of question marks and outputs a new dataframe\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        return pd.DataFrame([x.count('?') for x in df])\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_result(predicted, y):\n",
    "    \"\"\"\n",
    "    Takes predicted class and target class and returns a dict containing \n",
    "    evaluation statistics\"\"\"\n",
    "    \n",
    "    tp, tn, fp, fn = 0,0,0,0\n",
    "    stat = dict()\n",
    "\n",
    "    for y_hat, y_ in zip(predicted, y):\n",
    "        if y_hat == 1:\n",
    "            if y_ == 1:\n",
    "                tp +=1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if y_ == 1:\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn +=1\n",
    "    # (TP+TN)/(TP+TN+FP+FN)\n",
    "    accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "    # TP/(TN+FN)\n",
    "    precision = tp/(tp+fp)\n",
    "    # TP/(TP+FN)\n",
    "    recall = tp/(tp+fn)\n",
    "    # 2*recall*precision/(recall+precision)\n",
    "    f1 = 2*(recall * precision) / (recall + precision)\n",
    "\n",
    "    stat[\"Accuracy\"] = accuracy\n",
    "    stat[\"Precision\"] = precision\n",
    "    stat[\"Recall\"] = recall\n",
    "    stat[\"F1-Score\"] = f1\n",
    "    stat[\"TP\"] = tp\n",
    "    stat[\"TN\"] = tn\n",
    "    stat[\"FP\"] = fp\n",
    "    stat[\"FN\"] = fn\n",
    "\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with tests on standard distribution: 0.9403457952385263\n",
      "Accuracy with tests on distribution 87.1/12.9: 0.8898235347901619\n",
      "Accuracy with tests on tweets: 0.651615969581749\n"
     ]
    }
   ],
   "source": [
    "def get_model(model, vectorizer, qm, binary, stop_words, additional_settings, ngrams):\n",
    "    \"\"\"\n",
    "    Takes arguments on what model, vectorizer and settings should be used. \n",
    "    Outputs a pipeline with feature extractors and ml algorithm \n",
    "    \"\"\"\n",
    "\n",
    "    # ML model to use\n",
    "    if model == 'Naïve Bayes':\n",
    "        m = MultinomialNB()\n",
    "    elif model == \"Support Vector Machine\":\n",
    "        if additional_settings:\n",
    "            m = svm.SVC(kernel='linear', verbose=False, C=0.67)\n",
    "        else:\n",
    "            m = svm.SVC()\n",
    "    elif model == 'Random Forest':\n",
    "        if additional_settings:\n",
    "            m = RandomForestClassifier(n_estimators=500 ,criterion='entropy', random_state=0, verbose=0, n_jobs=8)\n",
    "        else:\n",
    "            m = RandomForestClassifier(random_state=0, verbose=0, n_jobs=8)\n",
    "    \n",
    "    # Feature Extractor\n",
    "    if vectorizer == 'CountVectorizer':\n",
    "        vec = CountVectorizer(stop_words=stop_words, binary=binary, ngram_range=ngrams)\n",
    "    elif vectorizer == \"HashingVectorizer\":\n",
    "        vec = HashingVectorizer(stop_words=stop_words, binary=binary, ngram_range=ngrams)\n",
    "    elif vectorizer == \"TfidfVectorizer\":\n",
    "        if additional_settings:\n",
    "            vec = TfidfVectorizer(stop_words=stop_words, binary=binary, smooth_idf=False, ngram_range=ngrams)\n",
    "        else:\n",
    "            vec = TfidfVectorizer(stop_words=stop_words, binary=binary, ngram_range=ngrams)\n",
    "\n",
    "    feature_union = [('vec', vec)]\n",
    "    # If question mark count should be considered as a feature.\n",
    "    if qm:\n",
    "        feature_union.append(('qm', QuestionMarkExtractor()))\n",
    "\n",
    "    ppl = Pipeline([\n",
    "        ('feats', FeatureUnion(feature_union)),\n",
    "        ('model', m)\n",
    "    ]) \n",
    "\n",
    "    return ppl\n",
    "\n",
    "models = ['Support Vector Machine' , 'Naïve Bayes' , 'Random Forest']\n",
    "vectorizers = [\"TfidfVectorizer\", 'CountVectorizer', \"HashingVectorizer\"]\n",
    "\n",
    "try:\n",
    "    # Getting the pipeline\n",
    "    model = get_model(\"Support Vector Machine\", \"TfidfVectorizer\", True, True, None, True, [1,2])\n",
    "    t0 = time.time()\n",
    "    # Fitting\n",
    "    model.fit(file_content[2][2], file_content[2][3])\n",
    "    t1 = time.time()\n",
    "    # Predictions on testset with standard distribution (12.9% negative)\n",
    "    y_hat = model.predict(file_content[0][5])\n",
    "    t2 = time.time()\n",
    "    fitting_time = t1-t0\n",
    "    result = eval_result(y_hat, file_content[0][6])\n",
    "    print(f\"Accuracy with tests on standard distribution: {result['Accuracy']}\")\n",
    "    # Predictions on testset with same distribution like it has been fitted with.\n",
    "    t1 = time.time()\n",
    "    y_hat = model.predict(file_content[2][5])\n",
    "    t2 = time.time()\n",
    "    result = eval_result(y_hat, file_content[2][6])\n",
    "    print(f\"Accuracy with tests on distribution 87.1/12.9: {result['Accuracy']}\")\n",
    "    # Predictions on Twitter dataset\n",
    "    t1 = time.time()\n",
    "    y_hat = model.predict(x_tweets)\n",
    "    t2 = time.time()\n",
    "    result = eval_result(y_hat, y_tweets)\n",
    "    print(f\"Accuracy with tests on tweets: {result['Accuracy']}\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2677f4823c0426748f459cdc95967bc765da36a10e49430f4108a27922fa1c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
